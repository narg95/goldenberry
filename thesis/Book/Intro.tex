\chapter{Introduction}
\label{ch:intro}

\section{Aim}

The main aim of the thesis was to design an algorithm for feature relevance estimation that discovers interactions among variables.  We aimed to build upon \WK~ method a newer version that incorporates a technique for bivariate feature selection within an population-based stochastic algorithm.  

\section{Problem and Motivation}
Feature subset selection (FSS) is a machine learning technique for dimensionality reduction in datasets where irrelevant or noisy variables may appear. It is useful in problems where few from a big set of observed variables explain a given pattern in classification, prediction or clustering of data cases (e.g. diagnosis of a disease, market analysis, image segmentation, etc.), and therefore it allows for an expert to focus on data that is really important, reducing the expenditure of costly or time-consuming experiments.  

To illustrate the problem, consider the situation where an insurance company is evaluating the factors that have an impact on people's overweight conditions that lead to claims in illness insurance policies. Upon registration, they collect information of both parents' weight and birthday. They want to know which of these variables are related to the development of the condition. Now let us suppose that one-year time is required by an insurance analyst to process the information of just one variable. Then it would be desirable to discard irrelevant variables in order to save him valuable time. An FSS method could help to select the relevant ones.

\begin{figure}[h]
	\centering
		\includegraphics{example01}
	\caption[Correlation analysis of input variables in relation to overweight conditions of insurance customers.]%
	{Correlation analysis of input variables in relation to overweight conditions of insurance customers (for the fictional example in the text).}
	\label{fig:im01}
\end{figure}

Figure \ref{fig:im01} depicts the result of applying a correlation-based feature selection method on this fictional problem; it is evident that half of the variables, namely those related to birth dates, are irrelevant and can be ignored for the problem of interest. Thus, dimensionality reduction may provide insights in data mining tasks, yielding a better understanding of objects or phenomena behavior and is useful to speed up data analysis and to improve generalization performance. For instance, in the previous insurance company example, Figure \ref{fig:im08} shows how sample cases can be now rendered in the 2D space obtained with the relevant variables; they can be easily discriminated with a simple hyperplane. By focusing on those two relevant variables, the analyst would have saved two precious years of working time and the company two equally valuable years of salary payments. 

\begin{figure}[h]
	\centering
		\includegraphics{example02}
	\caption{Using relevant features customers can be easily segmented with a simple linear function.}
	\label{fig:im08}
\end{figure}

Some FSS techniques assume variables are independent.  This assumption might not be appropriate for some real world problem since variables may influence others and these interactions are usually hidden.  A good example is in the field of bioinformatics where experts analyze proteins relevance in diseases to design inhibitory vaccines; using state-of-the-art high-throughput technologies (e.g. mass-spectometry\cite{lipton08}) they are able to collect large amounts of data about activation of protein mechanisms, but the information about how these interactions occur is unknown or difficult to obtain, not to mention that just a few proteins from a large proteomic spectrum would be related to the activation of the disease. 

The independent assumptions (univariate FSS methods, see section \ref{sec:xxx}) are very popular because of their low computational cost\cite{larranaga08} but they do not provide additional insights about data relationships.  On the other hand, missing information about those dependencies may affect negatively the prediction accuracy of the feature subset.  As an alternative approach, multivariate FSS methods search for feature subsets and possible dependency relationships between them at the cost of an increase in computational complexity (recall that FSS is a combinatorial problem known to be NP-Hard \cite{guyon03,rojas05}). The challenge is then to design novel feature selection methods that take advantage of multivariate power combined with high-accuracy classifiers to obtain improved prediction and explanatory performance.

On the other hand, kernel classifiers have recently emerged as powerful high-accuracy classifiers with robust generalization abilities \cite{cristianini04}.  They use linear functions to perform classification, allowing for non-linear discriminatory borderlines in the input space by means of a kernel function. This function is a mapping of similarity measures in a transformed space where nonlinearities can be solved linearly. Two widely used kernel functions are the RBF kernel and the polynomial kernel (Eq.(\ref{eq:eq01}) and Eq.(\ref{eq:eq02}) respectively):

\begin{equation}
	K_{\sigma}(\bar{x},\bar{z}) = \text{exp}\Big( -\sigma\sum_{k} (x_k-z_k)^2 \Big)
	\label{eq:eq01}
\end{equation} and
\begin{equation}
	K_d(\bar{x},\bar{z}) = \langle \bar{x}, \bar{z} \rangle^d,
	\label{eq:eq02}
\end{equation} where \(\bar{x}, \bar{z} \in X\), \(X \subseteq \mathbb{R}^D \) represents the input space and {\scriptsize $D$ } the number of variables or dimensions.  The parameter \(\sigma\) and \(d\) define the width of the RBF and polynomial kernel respectively.  Modified weighted versions of these kernels introduce scale factors \(\bar{w} = \{w_1...w_\ell\}\) to weight the amount that each dimension contributes to the total computation\cite{vapnik02}.  The weighted RBF and weighted polynomial kernels are then defined as Eq.(\ref{eq:eq03}) and (\ref{eq:eq04}):

\begin{equation}
	K_{\sigma}(\bar{x},\bar{z}) = \text{exp}\Big( -\sigma \sum_{k}^{\ell} w_k(x_k-z_k)^2 \Big)
	\label{eq:eq03}
\end{equation} and

\begin{equation}
	K_{d}(\bar{x},\bar{z}) = \Big( \sum_{k}^{\ell} w_k(x_k \cdot z_k) \Big)^d.
	\label{eq:eq04}
\end{equation}

Some FSS methods have been proposed in connection with kernel functions, for instance the weighted kernels in Eq. (\ref{eq:eq03}) and (\ref{eq:eq04}) have been used to define a FSS embedded method (the weigthed kernel iterative estimation of relevance algorithm, wKIERA\cite{rojas08}).  The idea in this case is to consider the weight vector \(\bar{w}\) of the kernel as a relevancy estimator of input variables. The vector is tuned by a univariate estimation of distribution algorithm \cite{larranaga01} which iteratively refines the distribution by a population-based technique inspired in genetic algorithms (GA)\cite{goldberg89}. The accuracy of the kernel classifier coupled with the weight vector candidates is taken as the fitness measure of the GA.  The authors showed promising results of this algorithm in selecting relevant variables in a number of different classification tasks, including problems with linear and non-linear hypothesis targets. This FSS method however, is designed on the basis of the independent assumption mentioned above and consequently does not provide the power of explanation of bivariate methods and does not account for information of relationship between the variables. One of the motivations of this thesis was to build upon this method and design a refined version that incorporates techniques for bivariate feature selection within an embedded population-based stochastic algorithm. A Hierarchical Tree was chosen to represent how variables depend and influence each other and a new algorithms, called \WKII, was develop to perform relevance estimation within such framework and to make feasible the approach. 

\section{Contributions}
The primary contributions of this thesis are:
\begin{itemize}
\item \mbox{\TILDA\cite{Rojas2012}}, a novel continuous \emph{compact} Estimation of Distribution Algorithm (\EDA) built upon the compact Genetic Algorithm (\cGA) and the continuous domain Population-Based Incremental Learning algorithm.
\item \mbox{\GB\cite{Rojas2013}}, a supplementary machine learning and evolutionary computation suite of components for \Orange. This suite provides components in the form of \EDA (\cGA\cite{Harik99}, \UMDA\cite{Muhlenbein97}, \PBIL\cite{Baluja95}, \TILDA\cite{Rojas2012}, \PBILc\cite{Sebag98}, \UMDAc\cite{Larranaga02}, \BMDA\cite{Pelikan99}), Machine Learning (\KPerceptron, \SVM~adaptation), Feature Selection (\BFSS, \WKCostFunc), and a set of utility components (\CostFunction~, \KBuilder, \BBTester).
\item An enhanced version of \BMDA, able to uses different kind of dependence scoring functions like Mutual Information, Pearson's chi-squared test and the Combined Mutual-Information p-value (SIM\cite{?}) method.
\item \WKII, an new feature relevance estimation algorithm build upon its previous version \WK.  This new version considers the existence bivariate relationships among variables. It uses the \BMDA~ for estimating bivariate dependencies and features relevances, and the classification accuracy from a learning algorithm (\KPerceptron~or a \SVM) as the cost function to assess the goodness of the search space.
\end{itemize}
\section{Literature Review}
\label{sec:lecrev}
In this section we provide a summarized review of the main elements involved in this thesis, namely feature selection techniques, kernel classification machines, and estimation of distribution algorithms.

\subsection{Feature Selection Techniques}
\label{sec:feat1}
\subsubsection{The problem of feature selection}
\label{sec:feat11}
Nowadays the advances in technologies for data collection (the genome project, particle colliders, internet-based social networks) pose increased challenges for data analysis due to larger sizes and higher dimensionality of the observed samples.  It is reasonable however to assume that the collected variables may exhibit redundancy, inconsistency, noisy and irrelevant data and therefore will be difficult to analyze by the human eye. New automated mechanisms are required to identify significant variables for pattern discovery and data mining.

Feature selection is gaining in importance and has recently become an active field of research in disciplines such as knowledge discovery, machine learning, pattern recognition, bioinformatics, geoinformatics, etc.  While FSS are techniques for dimensionality reduction, they maintain the original variables compared to other entropy-based, data compression or statistical methods that modify the original data representation instead of providing a subset of significant features with useful information for the domain experts \cite{larranaga07}. The main goal of these techniques is to obtain a better understanding of the data for visualization purposes or to speed up data analysis.

FSS techniques are helpful in improving prediction models for supervised learning, detecting dimensions of tight data conglomeration in clustering tasks and a deeper understanding of process for data generation. Nonetheless, the design of novel FSS techniques aimed to provide more useful information also incurs in new levels of complexity giving rise to new challenges pertaining to feasible and efficient computational techniques.  

In the classification context, feature selection techniques are categorized in three main groups depending on how they interact with a classification model (see Figure ~\ref{fig:im09}):  filters, wrappers and embedded. A detailed description of this categorization is given below (based on the study in \cite{larranaga08}).

\begin{figure}[ht]
	\centering
		\includegraphics[scale=0.8]{fss}
	\caption[A feature selection taxonomy.]%
	{A feature selection taxonomy.  The feature selector category depends on how the feature and hypothesis space are combined during classifier learning (left and right box taken from \cite{larranaga07}).}
	\label{fig:im09}
\end{figure}

\subsubsection{Filter Approach}
Methods in this category assess the relevance of variables based on the intrinsic properties of data.  The search in feature space is separated from the search in the space of classification hypothesis.  In most cases a feature relevance score is calculated (i.e. using mutual-information or data correlation) and low-scoring features are removed.  These methods have the ability to scale to higher dimensions since they exhibit a low computational cost.
\subsubsection{Wrapper Approach}
Wrappers establish a symbiotic relationship between the feature subset search and the classification algorithm.  The aim of these type of methods is not only to find relevant features but also to find the best suited classification model to those features.  However the interaction with the classifier may induce new problems such as a higher risk of overfitting and a higher demand of computational resources. 
\subsubsection{Embedded Approach}
Embedded methods incorporate the feature selection and the classification model construction as a single process.  As a result feature and hypothesis spaces are combined providing a richer source of information during the search although incurring as well in additional computational costs.  
\subsection{Machine Learning}
Supervised learning aims to find optimal learning algorithms with high generalization and prediction performance to produce classification models of the training set of examples and their associated labels.  The prediction accuracy of the resulting classifier is then evaluated with a test set.  Among these techniques, linear classifiers are widely used because of their theoretically simplicity and computational efficiency.  Application of linear classifiers, in real world problems where nonlinearities arise, has been possible due to the advances in kernel machines\cite{cristianini04}.
\subsubsection{Kernel Machines}
\label{subsub:kern}
These algorithms use kernel functions to compute similarity measures of the input samples in a feature space where nonlinearities might be easier to solve by linear classifiers.  Figure ~\ref{fig:im10} shows an example of feature mapping where in the original space data can only be separated with a nonlinear function but becomes linearly separable if the data is transformed into a feature space.  When using the linear classifier the feature space mapping is implicit since the machine only uses inner products of the transformed examples which are computed using kernel function.   

\begin{figure}[ht]
	\centering
		\includegraphics[scale=1.2]{kernel}
	\caption[Transformation from input space to a feature space simplifies the classification task.]%
	{Transformation from input space to a feature space simplifies the classification task (taken from\cite{cristianini04}).}
	\label{fig:im10}
\end{figure}

A kernel is a function \(K\) such that for all \(\bar{x},\bar{z} \in X\) \[K(\bar{x},\bar{z})=\langle \phi(\bar{x}) ,\phi(\bar{z}) \rangle,\] where \(\phi\) is a mapping from the input space \(X\) to an (inner product) feature space. In order to be a kernel, the function must comply with the conditions defined by Mercer's theorem \cite{cristianini04}.  Eq. (1-4) are examples of kernel functions.

\subsubsection{Linear Learning Machines}
A linear classifier is a linear function \(f(\bar{x})\) where \(\bar{x} \in X\) and \(\bar{w} \in \mathbb{R}^\mathcal{D}\), that can be written as:

\begin{equation}
	f(\bar{x})=\text{sgn}(\langle \bar{w}, \bar{x} \rangle) = \text{sgn} \Big( \sum_{i} w_i x_i\Big)
\label{eq:eq05}
\end{equation}

A given sample \(\bar{x}\) is assigned to the positive class if \(f(\bar{x})=0\) or otherwise to the negative class.  There are number of training algorithms to learn a classification vector \(\bar{w}\), including the perceptron\cite{rosenblatt58} and the linear support vector machine\cite{cristianini00}.  For illustration purposes, Algorithm ~\ref{alg:percep} shows the learning procedure of the perceptron.

The kernelized version of the linear classifier allows for classification of nonlinearly separable datasets, as mentioned before.  The classification function is written consequently as:

\begin{equation}
	f(\bar{x}) = \text{sgn}\Big( \sum_{k} \alpha_k K(\bar{x_k},\bar{x}) \Big),
\label{eq:eq06}
\end{equation}

where \(K\) represents a kernel function and \(\alpha_k\) the classifier parameters that can be learned in this case using the kernel perceptron\cite{freund99} or the support vector machine\cite{vapnik95, cristianini00}. 

\begin{algorithm}[ht]
	\caption{\textsf{The Perceptron}} 
	\begin{algorithmic}
		\REQUIRE Given a linearly separable training set $S$ and learning rate $\eta \in \mathbb{R}^+$ 
		\STATE $w \gets 0$; $k \gets 0$
		\REPEAT 
				\FOR{ $i=1, 2, \ldots \ell$}
					\IF{$y_i (\langle w_k, x_i \rangle) \leq 0$}
						\STATE $w_{k+1} \gets w_k + \eta y_i x_i$
						\STATE $k \gets k + 1$
					\ENDIF
				\ENDFOR				
		\UNTIL{no mistake made within the \emph{for} loop}
		\ENSURE $(w_k)$ where $k$ is the number of mistakes
	\end{algorithmic}
  \label{alg:percep}
\end{algorithm}

\subsection{Evolutionary Estimation  of Probabilistic Distributions}
Genetic algorithms (GA) are search stochastic methods inspired in the theory of natural selection of Darwin.  The idea is to evolve a population of candidates coding the parameters for the solution of an optimization problem, using genetic operations such as chromosome recombination and mutation \cite{goldberg89}. A novel technique known as Estimation of Distribution Algorithms (EDAs) has recently emerged motivated by GAs but from a statistical viewpoint. They have proven to be better suited in many applications than canonical GAs\cite{larranaga01}.  

The main distinctive aspect of EDAs is that they search for a probabilistic distribution model representing the population of candidates. Instead of using genetic operations, these algorithms are based on well-known statistical techniques to estimate the parameters of a distribution function, and the evolution is guided by sampling the evolving probabilistic model. The complexity of the algorithm lies in the robustness of this probability model and in how it is iteratively re-estimated.  The probabilistic models can be as simple as a marginal distribution or as complex as a joint multivariate distribution expressing high order interactions among the observed variables. These categories are briefly described below following the review in \cite{larranaga08}.

\subsubsection{Univariate EDAs}
Univariate algorithms use a marginal probability model encoded in a probability vector.  They are not computational intensive because they assume no interaction between variables. The probability vector is re-estimated using the fittest subset of the population of candidates, as depicted in Figure ~\ref{fig:im04}. There are three widely used algorithms for this category: Univariate Marginal Distribution Algorithm (UMDA\cite{muhlen96}), Popupation-based Incremental Learning (PBIL\cite{baluja94}) and the Compact Genetic Algorithm (cGA\cite{baluja95, harik98}).  

\begin{figure}[ht]
	\centering
		\includegraphics{edas}
	\caption{Univariate Estimation of Distribution Algorithm flowchart.}
	\label{fig:im04}
\end{figure}

UMDA estimates the entire probability vector every iteration.  The probability distribution is factorized as a product of independent univariate marginal distributions, which are estimated from marginal frequencies.  The probability of the \(i\)-th variable being relevant is given by the Eq.(\ref{eq:eq07}), where \(S\) is the subpopulation of \(N\) fittest candidates, and \(\delta\) is the Kronecker delta.

\begin{equation}
	p(x_i)= \prod_{j} \frac {\sum_{j}^{N} \delta(X_i=x_i|S)} {N}
\label{eq:eq07}
\end{equation}

PBIL was designed to work in a \(n\)-dimensional binary space \(\{0,1\}^n\).  Unlike UMDA, PBIL does not estimate a new probability vector every iteration \(t\). Instead, fittest candidates are chosen to update the probability vector at a learning rate \(\alpha=(0,1]\). The updating rule is shown in Eq.(\ref{eq:eq08}). Notice that PBIL becomes UMDA when \(\alpha=1\). 

\begin{equation}
	p_{t}(x_i)= (1-\alpha)p_{t-1}(x_i)+\alpha \frac{1}{N} \sum_{k}^{N} S_{ki} 
\label{eq:eq08}
\end{equation}


Lastly, cGA has become popular for the higher efficiency to solve very large scale problems with millions to billions of variables with a lower computational demand than canonic GA\cite{goldberg07}.  cGA has a low memory consumption because only two candidates per iteration are generated.  Both compete and the winner updates the probability vector at a learning rate \(1/n\) where \(n\) is the population size parameter. 
Algorithm \ref{alg:cga} shows the pseudocode of the cGA .

\begin{algorithm}[ht]
	\caption{\textsf{The Compact Genetic Algorithm}} 
	\begin{algorithmic}
		\REQUIRE Population size $n$ and chromosome lenght $\ell$
		\STATE $p \gets$ initialize a uniform probability vector.
		\REPEAT
			\STATE $[a,b] \gets$ generateTwoIndividuals$(p)$
			\STATE $[winner, loser] \gets $compete$(a,b)$
			\FOR { $i=1, 2, \ldots \ell$}
				\IF {$winner[i] \neq loser[i]$}
					\IF{$winner[i] = 1$}
						\STATE $p[i] \gets p[i] + \frac{1}{n}$
					\ELSE
						\STATE $p[i] \gets p[i] - \frac{1}{n}$
					\ENDIF
				\ENDIF
			\ENDFOR
	\UNTIL $p$ has converged
	\ENSURE $p$ represents the final solution
	\end{algorithmic}
	\label{alg:cga}
\end{algorithm}
\subsubsection{Bivariate EDAs}
Bivariate \EDA s use joint statistics models of 2-order-dependencies to represent interaction among variables.  Most representative algorithms are: the Mutual-Information Maximizing Input Clustering (MIMIC\cite{?}), the Combining Optimizers with Mutual Information Trees algorithm (COMMIT\cite{?}) and the  Bivariate Marginal Distribution Algorithm (BMDA\cite{?}). All of them focus their attention in finding hidden pair-wise interactions among variables assuming different interaction's forms as depicted in Figure \ref{fig:imbedas}. MIMIC was one of the first attempts for a bivariate \EDA~. MIMIC assumes pair-wise interactions in a chain-like form where each variable is conditioned to the previous one. MIMIC iteratively estimates marginal and conditional probabilities using the promising candidates and through a greedy algorithms tries to find a optimal chain of variables that better minimizes the cross-entropy.   COMMIT behaves similar to MIMIC but instead of using a chain-like model, it tries to find a suitable tree representation of variables.  The tree is estimated on each iteration using the promising candidates through a maximum spanning tree algorithm and a graph where variables represent nodes and edges the Mutual Information between nodes. Therefore COMMIT assumes all variables as been connected; whereas BMDA, on the other hand, behaves similar to COMMIT but it assumes that some variables may be independent.  BMDA uses a maximum spanning forest algorithm for inferring dependencies from the graph of variables and the \emph{Pearson's chi-squared test} to determine whether edges exists between nodes.

\begin{figure}[ht]
	\centering
		\includegraphics{bedas}
	\caption{Diagram of probability models used in most popular bivariate EDAs.}
	\label{fig:imbedas}
\end{figure}


\subsubsection{Multivariate EDAs}
Multivariate EDAs use joint statistics models of higher order to represent interaction among variables. These models are usually represented as probabilistic graphical models (see Figure \ref{fig:im05}). As mentioned above, EDA's complexity increases when a higher order of interaction among variables are desired. Factorized Distribution Algorithm (FDA \cite{muhlen99}), Estimation of Bayesian Networks Algorithm (EBNA \cite{larranaga00}) and Bayesian optimization algorithm (BOA \cite{pelikan99}) belong to this category.  BOA and EBNA both use Bayesian network structures but differs in the score metric they use to select the appropriate network structure.  BOA uses Bayesian Dirichlet equivalence score (BDe) while EBNA uses K2+Penalization and Bayesian Information Criterion (BIC).  Similarly, FDA uses Boltzmann selection for Boltzman distribution.

\begin{figure}[ht]
	\centering
		\includegraphics{medas}
	\caption[Diagram of probability models used in most popular multivariate EDAs.]%
	{Diagram of probability models used in most popular multivariate EDAs( taken from \cite{larranaga08}).}
	\label{fig:im05}
\end{figure}

 
\subsection{Feature Selection Techniques Using Estimation of Distribution Algorithms}
Estimation of Distribution Algorithms for feature subset selection and feature relevance estimation has been proposed with promising results in large scale domains such as bionformatics \cite{inza00,saeys03}. Among these techniques, particularly FSS-EBNA \cite{larranaga00FSS} and wKIERA\cite{rojas08} have been used for selection of relevant feature subsets.  
FSS-EBNA uses a multivariate estimation of distribution algorithm (EBNA) as the search engine for exploring the feature space and a Naive-Bayes classifier (NB) to predict the class for each instance; Figure \ref{fig:im06} illustrates the main components for this algorithm.

\begin{figure}[ht]
	\centering
		\includegraphics[scale=1.3]{FSSEBNA}
	\caption{Main components of the FSS-EBNA algorithm.}
	\label{fig:im06}
\end{figure}
 
On the other hand wKIERA uses a univariate estimation of distribution algorithm for searching the best suited representation of features relevance and a RBF weighted kernel machine with a perceptron classifier as the learning algorithm; Figure \ref{fig:im07} illustrates the main components of this algorithm.

\begin{figure}[ht]
	\centering
		\includegraphics[scale=1.3]{wkiera.png}
	\caption{Main components of the wKIERA algorithm.}
	\label{fig:im07}
\end{figure}
